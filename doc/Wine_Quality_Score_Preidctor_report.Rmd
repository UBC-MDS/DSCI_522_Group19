---
title: "Predicting wine quality score from various characteristics"
author: "Group 19 - Kingslin Lv, Manju Neervaram Abhinandana Kumar, Zack Tang, Paval Levchenko"
bibliography: references_wine_score_predictor.bib
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(feather)
library(arrow)
library(knitr)

# path <- "../results/tuned_crossval_results.feather"
# df <- read_feather(path)
```

## Summary

In this project we aim to predict the wine quality scores ranging from 0 to 10 based on physicochemical properties of wines sensory tests. To answer this predictive question, we decide to build a regression model. Through our exploratory data analysis we analyze the distribution of each feature and correlation between features and the target. Then through the cross-validation process based on feature input, we concluded that the Random Forest model delivers a much higher training score, but there was a clear problem of overfitting. We further conduct feature selection and hyperparameter optimization in an attempt to reduce the score gap between train and test data. We are able to drop number of features but maintain the relatively similar score through this process. Unfortunately, the test score with the best hyperparameters was only around 0.53, which is fairly acceptable. Next we potentially can improve our model prediction score by using a larger dataset with more features and find a more high score model with its best hyperparameters.

## Introduction

The wine industry shows a recent extensive growth and the industry experts are using product quality certifications to promote their products[@orth2001quality]. This is a time-consuming process and requires the assessment given by human experts, which makes this process very expensive. The wine market would be of interest if the human quality of tasting can be related to wine's chemical properties so that quality assessment processes are more controlled. This project aims to build up a machine learning model for purpose of predicting the wine quality score based on specific chemical properties of each beverage. This task will likely require a lot of domain knowledge and according to a paper published by Dr. P. Cortez, Dr. A. Cerdeira, Dr. F. Almeida, Dr. T. Matos and Dr. J. Reis they were able to demonstrate the results of a data mining approach had promising results compared to alternative neural network methods [@CORTEZ2009547].

This model is useful to support wine tasting evaluations. Quality evaluation is part of wine certification process and can be used to improve wine making and classify wines to premium brands which can be useful for setting prices and for marketing purposes based on consumer tastes. It should be noted that using taste as a sensory measurement for wine quality could be quite unreliable. We are also interested in exploring how much output data could depend on other sensory information such as color of wine. Potentially, human brain could be processing taste and visual information differently rather than taste only. Thus, we are not expecting to obtain a really high test score.

# Methods

## Data

The dataset used in this project was retrieved from the University of California Irvine (UCI) machine learning repository [@Dua2019] and was collected by Paulo Cortez, University of Minho, Guimarães, Portugal and A. Cerdeira, F. Almeida, T. Matos with help from J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal in 2009. This dataset contains the results of various physiochemical test, including scoring for properties like fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol (11 features), which were preformed on white "Vinho Verde" wine samples from Northern Portugal. The data used in our analysis can be found [here](https://archive.ics.uci.edu/ml/datasets/wine+quality). Additionally, we add one more feature by concatenating white and red wine data, and so there is a binary feature; we think potentially human's perception of wine type may affect the independent scoring on the wine quality, thus, we added a binary feature to account for this factor [@legin2003evaluation].

No additional features or specific branding of each wine is available in the dataset for privacy purposes. Each row in the dataset represents a single wine which was tested and scored based on human sensory data.

## Analysis

As the first step towards building the model to answer the predictive question posed above we split the data into train and test data set at 80% and 20% level. We perform our exploratory data analysis on the training data frame. Firstly we plotted the distribution of the quality scores for each wine (Figure 1). Despite the quality scoring being performed a scale from 1-10 only values in the range of 3-9 were observed. It can be seen that our data is significantly imbalanced, with 6 being the most common score observed across all testing while scores such as 3 and 9 were rarely seen.

```{r fig_1, echo=FALSE, fig.cap="Figure 1. Distribution of quality scores", out.width = '30%'}
knitr::include_graphics("../results/quality_dist.png")
```

After taking a look at the distributions of our 11 numerical features, and we realize all attributes have outliers with extreme value. We will have to remove the outliers and reduce skewness. Also, there is a class imbalance issues as revealed from the distribution plot.

```{r fig_2, echo=FALSE, fig.cap="Figure 2. Data distribution of numeric features in training datasets.", out.width = '100%'}
knitr::include_graphics("../results/repeat_plots.png")
```

By exploring the features correlation matrix, we identified that some features are highly correlated, and we may choose to drop some redundant features in the process of feature selection. By the below correlation matrix, volatile.acidity, sulphates and alcohol are the attributes most coorelated with quality of wine. Thus, these 3 attributes are most attractive to me. We might drop some features that have smaller correlations such as fixed acidity and type. We will further identify this through our model establishment process.

```{r fig_3, echo=FALSE, fig.cap="Figure 3. Quality distribution of wines in the training and test datasets.", out.width = '60%'}
knitr::include_graphics("../results/cor_plot.png")
```

Following the `ColumnTransformer` process, we will dive deep into the class imbalance issue we figured out in the EDA process and add class_weight argument in the model. We attempt to utilize four different models to fit the training data set including `Ridge`, `OneVsRestClassifier`,`SVC`, and `RandomForestRegressor`. By applying cross-validation techniques and comparing validation and train scores, we are going to select the most well-performed model and conduct hyper-parameter optimization accordingly[@Python], [@scikit-learn]. All models built in our analysis were fit using all of the variables from the dataset. Hyperparameters `n_estimators`, `max_leaf_nodes`, and `max_depth` were optimized via random search while all other hyperparameters used the default sklearn `RandomForestRegressor` values. The data was processed using the pandas package and EDA was performed using the pandas-profiling package [@reback2020pandas] [@pandasprofiling2019]. This document was compiled using an R document file with scripts run using the docopt package [@R], [@docopt]. Tables were stored using feather files (with dependency on arrow) and displayed using knitr's kable function [@feather], [@arrow], [@knitr]. This document was compiled using rmarkdown [@rmarkdown]. After tuning the model, we will ready to use test data set to do the final check of the accuracy. If the result is not satisfactory, we will make further adjustments based on the new issue found.

# Results & Discussion

After we decided to approach our problem as regression issue, we chose to use four typical regression supervised learning models, named as Ridge, SVC, OneVsRestClassifier and RandomForestRegressor. And for comprehensive comparison purpose, we included negative mean squared error, negative root mean squared error, negative mean absolute error, r squared and MAPE scorer to see the performance of our model. The cross-validation scores and train scores for each model is shown in the Table x (Out(234) in model fitting raw code).

By assessing the numbers in the table, we discovered that Random Forest returned the highest cross-validation scores, so we decided to proceed with Random Forest model for further feature selections and hyper-parameter optimization.

```{r table_1, echo=FALSE, out.width = '60%'}
```

We applied Recursive Features Elimination (RFE) for preliminary feature selections, and limit the number of features as 10 in order to make the model more efficient. By using the algorithm, we chose to drop "type" and "fixed acidity" features. Because by dropping those two features, even though we did not get better scores, we are able to achieve the same scores with lesser features. It is great because it will simplify our model and save cost for future data collection. The result of the updated scores is shown in the Table y (Out(259) in model fitting raw code).

```{r table_2, echo=FALSE, out.width = '60%'}

```

Finally, we conduct hyper-parameter optimization because we recognized that Random Forest method encountered severe overfitting issue. We employed random search to look for some key hyper-parameters, such as max_depth, max_leaf_nodes, and n_estimators. The best hyper-parameter value we got from the algorithm are \['max_depth': 344, \['max_leaf_nodes': 851, 'n_estimators': 258\]. And the best validation score we got is 0.48. And the test score is 0.52 after tunning the hyper-parameters. However, as we discovered above, the train score is 0.91, which means we still have overfitting issue by using Random Forest model. We may investigate further and figure out the next steps later. The final best scores are shown in the Table z (Out(277) in model fitting raw code).

```{r table_3, echo=FALSE, out.width = '60%'}
```

# Limitations & Future

The wine classification is a difficult task as it relies on sensory analysis performed by human tasters. These evaluations are based on the experience and knowledge of experts which are prone to subjective factors. One of main limitation here is that the dataset is imbalanced. The majority of quality scores were 5 and 6. In order, to improve the predictive model, need to have a balanced data. Another limitation is that the dataset has only 11 features. A solution to this, could be to add more relevant features like grape type, year , wine brand etc.If we have more samples and data was balanced, we can try other machine learning techniques and build better model with good results.

# References
